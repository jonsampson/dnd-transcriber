# WhisperX Configuration
WHISPERX_MODEL=large-v2
WHISPERX_DEVICE=cuda
WHISPERX_COMPUTE_TYPE=float16
WHISPERX_LANGUAGE=

# Ollama Configuration
OLLAMA_MODEL_NAME=mistral-nemo:12b-instruct-2407-fp16
OLLAMA_API_URL=http://localhost:11434
OLLAMA_TEMPERATURE=0.1

# Demucs Configuration
DEMUCS_MODEL=htdemucs
DEMUCS_DEVICE=cuda
DEMUCS_SEGMENT_LENGTH=

# Pipeline Configuration
OUTPUT_FORMAT=json